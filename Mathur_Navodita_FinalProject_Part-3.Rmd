---
title: "PPG Paint Colors: Final Project"
subtitle: "Part 3: Classification"
author: "Navodita Mathur"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This RMarkdown shows how to predict probability of binary outcome in the final project data.   

## Load packages

```{r, load_packages}
library(tidyverse)
```

## Read data

The code chunk below reads in the final project data.  

```{r, read_final_data}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

The `readr::read_csv()` function displays the data types and column names associated with the data. However, a glimpse is shown below that reveals the number of rows and also shows some of the representative values for the columns.  

```{r, show_data_glimpse}
df %>% glimpse()
```

The data consist of continuous and categorical inputs. The `glimpse()` shown above reveals the data type for each variable which state to you whether the input is continuous or categorical. The RGB color model inputs, `R`, `G`, and `B` are continuous (dbl) inputs. The HSL color model inputs consist of 2 categorical inputs, `Lightness` and `Saturation`, and a continuous input, `Hue`. Two outputs are provided. The continuous output, `response`, and the Binary output, `outcome`. However, the data type of the Binary outcome is numeric because the Binary `outcome` is **encoded** as `outcome = 1` for the EVENT and `outcome = 0` for the NON-EVENT.  

# Binary classification task

The Binary output variable, `outcome`, is a numeric variable.  

```{r, show_outcome_class}
df %>% pull(outcome) %>% class()
```

However, there are **only** two unique values for `outcome`.  

```{r, show_outcome_values}
df %>% dplyr::count(outcome)
```

Here `outcome = 1` denotes the **EVENT** while `outcome = 0` denotes the **NON-EVENT**. Thus, the `outcome` variable uses the 0/1 encoding! The below code chunk removes the `response` variable to focus on the inputs and binary outcome.  

```{r, make_iiiA_data}
dfiiiA <- df %>% 
  dplyr::select(-response)

dfiiiA %>% glimpse()
```


Let us have a look if it is a roughly balanced dataset.
```{r}
mean(dfiiiA$outcome == 1)
```

# iii A)

Load the library to visualize the roc plot
```{r}
library(Deducer)
```

## Model Construction

### Model-1 

 The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a intercept only model. The result is assigned to the `glm_mod01` object.  
```{r}
Xmat_glm_01 = model.matrix(outcome~1, data = dfiiiA)
glm_mod01 <- glm(outcome~1, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod01)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod01)
```

```{r}
glm_mod01$coefficients
```

### Model-2

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a additive Category inputs model. The result is assigned to the `glm_mod02` object.
```{r}
Xmat_glm_02 = model.matrix(outcome~Lightness+Saturation, data = dfiiiA)
glm_mod02 <- glm(outcome~Lightness+Saturation, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod02)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod02)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod02)
```

### Model-3

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a additive Continuous inputs model. The result is assigned to the `glm_mod03` object.
```{r}
Xmat_glm_03 = model.matrix(outcome~R+G+B+Hue, data = dfiiiA)
glm_mod03 <- glm(outcome~R+G+B+Hue, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod03)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod03)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod03)
```

### Model-4

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a additive inputs model. The result is assigned to the `glm_mod04` object.
```{r}
Xmat_glm_04 = model.matrix(outcome~., data = dfiiiA)
glm_mod04 <- glm(outcome~., data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod04)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod04)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod04)
```

### Model-5

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with Interaction of the categorical inputs with all continuous inputs main effects. The result is assigned to the `glm_mod05` object.
```{r}
Xmat_glm_05 = model.matrix(outcome~(R+G+B+Hue)*Lightness + (R+G+B+Hue)*Saturation, data = dfiiiA)
glm_mod05 <- glm(outcome~(R+G+B+Hue)*Lightness + (R+G+B+Hue)*Saturation, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod05)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod05)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod05)
```

### Model-6

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with addition of categorical inputs to all main effect and all pairwise interactions of continuous inputs. The result is assigned to the `glm_mod06` object.
```{r}
Xmat_glm_06 = model.matrix(outcome~Lightness + Saturation + (R+ G+B + Hue)^2, data = dfiiiA)
glm_mod06 <- glm(outcome~Lightness + Saturation + (R+ G+B + Hue)^2, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod06)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod06)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod06)
```

### Model-7

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with interaction of categorical inputs to all main effect and all pairwise interactions of continuous inputs. The result is assigned to the `glm_mod07` object.
```{r}
Xmat_glm_07 = model.matrix(outcome~(Lightness+Saturation)*((R + G + B + Hue)^2), data = dfiiiA)
glm_mod07 <- glm(outcome~(Lightness+Saturation)*((R + G + B + Hue)^2), data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod07)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod07)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod07)
```

### Model-8

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with interaction of all splines of degree 2 of continuous variables G, B and Hue added to Saturation. The result is assigned to the `glm_mod08` object.
```{r}
Xmat_glm_08 = model.matrix(~splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2) + (Saturation), data = dfiiiA)
glm_mod08 <- glm(outcome~ splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2) + (Saturation), data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod08)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod08)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod08)
```

### Model-9

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with addition of all splines of degree 2 of continuous variables with categorical inputs. The result is assigned to the `glm_mod09` object.
```{r}
Xmat_glm_09 = model.matrix(outcome~splines::ns(R, df=2)+splines::ns(G, df=2)+splines::ns(B, df=2)+splines::ns(Hue, df=2)+Lightness+Saturation, data = dfiiiA)
glm_mod09 <- glm(outcome~splines::ns(R, df=2)+splines::ns(G, df=2)+splines::ns(B, df=2)+splines::ns(Hue, df=2)+Lightness+Saturation, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod09)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod09)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod09)
```

### Model-10

The code chunk below constructs a design matrix and fits a linear model to predict binary outcome via a model with addition of all splines of degree 5 of continuous variables with categorical inputs. The result is assigned to the `glm_mod010` object.
```{r}
Xmat_glm_10 = model.matrix(outcome~splines::ns(R, df=5)+splines::ns(G, df=5)+splines::ns(B, df=5)+splines::ns(Hue, df=5)+Lightness+Saturation, data = dfiiiA)
glm_mod10 <- glm(outcome~splines::ns(R, df=5) + splines::ns(G, df=5) + splines::ns(B, df=5) + splines::ns(Hue, df=5) + Lightness + Saturation, data = dfiiiA, family = 'binomial')
```

```{r}
summary(glm_mod10)
```

Visualize the rocplot for the constructed model.
```{r}
rocplot(glm_mod10)
```

Visualize the coefficient plot for the constructed model.
```{r}
coefplot::coefplot(glm_mod10)
```

## Model Comparison

The code chunk below compares the performance above models using AIC and BIC
```{r}
perf_metrics <- function(mod, model_name)
{
  broom::glance(mod) %>% 
    mutate(model_name = model_name)
}

model_list <- list(glm_mod01,glm_mod02,glm_mod03,glm_mod04,glm_mod05,glm_mod06,glm_mod07,glm_mod08, glm_mod09, glm_mod10)
model_names <- list("mod01","mod02","mod03","mod04","mod05","mod06","mod07","mod08","mod09","mod10")
all_model_metrics <- purrr::map2_dfr(model_list,
                                     model_names,
                                     perf_metrics)

all_model_metrics %>% dplyr::select(model_name, logLik, AIC, BIC)
```

```{r}
all_model_metrics %>% 
  dplyr::select(model_name, AIC, BIC) %>% 
  pivot_longer(!c("model_name")) %>% 
  ggplot(mapping = aes(x = model_name, y = value)) +
  facet_wrap(~name) +
  geom_point() +
  theme_bw()
```

Mod 8, 10, 6 are selected as top 3 models 

```{r}
coefplot::coefplot(glm_mod08)
```

```{r}
coefplot::coefplot(glm_mod10)
```

```{r}
coefplot::coefplot(glm_mod06)
```

Hue,G are selected as top variables
8th model i.e. `glm_mod08` is selected as the best model and 10th model i.e.`glm_mod10` is selected as the 2nd best model as it has best performance after 8 and takes all variables with splines as well as categorical variables into consideration

# iii B)

Load the essential libraries
```{r}
library(bayesrules)
```

Prepare info for the bayesian models
```{r}
info_glm_08 <- list(
  yobs = dfiiiA$outcome,
  design_matrix = Xmat_glm_08,
  mu_beta = 0,
  tau_beta = 4.5
)

info_glm_10 <- list(
  yobs = dfiiiA$outcome,
  design_matrix = Xmat_glm_10,
  mu_beta = 0,
  tau_beta = 4.5
)
```

The below code constructs Logpost function to calculate the log likeihood of a model binomial with binary outcome
```{r}
glm_logpost <- function(unknowns, my_info)
{
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- X %*% as.matrix(unknowns)
  
  # calculate the event probability
  mu <- boot::inv.logit(eta)
  
  # evaluate the log-likelihood
  log_lik <- sum(dbinom(x = my_info$yobs,size = 1, prob = mu,log = TRUE))
  
  # evaluate the log-prior
  log_prior <- sum(dnorm(x = unknowns,mean = my_info$mu_beta, sd = my_info$tau_beta, log = TRUE))
  
  # sum together
  return(log_lik+log_prior)
}
```

The below code constructs laplace function to find optimum value of the above function.
```{r}
glm_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 10001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```

Perform the Laplace Approximation for the selected models
```{r}
laplace_glm_08 <- glm_laplace(rep(0,ncol(info_glm_08$design_matrix)), glm_logpost, info_glm_08)
```

```{r}
laplace_glm_10 <- glm_laplace(rep(0,ncol(info_glm_10$design_matrix)), glm_logpost, info_glm_10)
```

Calculate the posterior model weight associated with each of the models
```{r}
bayes_glm_08 <-exp(laplace_glm_08$log_evidence)/sum(exp(laplace_glm_08$log_evidence),exp(laplace_glm_10$log_evidence))
bayes_glm_10 <-exp(laplace_glm_10$log_evidence)/sum(exp(laplace_glm_08$log_evidence),exp(laplace_glm_10$log_evidence))
```

Prepare bar chart
```{r}
Bayes <- c(bayes_glm_08,bayes_glm_10)
Names <- c("bayes_glm_08","bayes_glm_10")

df_model <- data.frame(Bayes, Names)

df_model %>% ggplot() +
  geom_bar(aes(x=Names,y=Bayes),stat="identity")
```

The code chunk below Visualizes the regression coefficient posterior summary statistics for your best model

```{r}
viz_post_coefs <- function(post_means, post_sds, xnames)
{
  tibble::tibble(
    mu = post_means,
    sd = post_sds,
    x = xnames
  ) %>% 
    mutate(x = factor(x, levels = xnames)) %>% 
    ggplot(mapping = aes(x = x)) +
    geom_hline(yintercept = 0, color = 'grey', linetype = 'dashed') +
    geom_point(mapping = aes(y = mu)) +
    geom_linerange(mapping = aes(ymin = mu - 2 * sd,
                                 ymax = mu + 2 * sd,
                                 group = x)) +
    labs(x = 'feature', y = 'coefficient value') +
    coord_flip() +
    theme_bw()
}
```

```{r}
viz_post_coefs(laplace_glm_10$mode,sqrt(diag(laplace_glm_10$var_matrix)),colnames(info_glm_10$design_matrix))
```

# iii C

The code chunk below creates synthetic data to visualize the prediction trends
```{r}
viz_grid <- expand.grid(R = seq(min(dfiiiA$R), max(dfiiiA$R), length.out=6),
                        G = seq(min(dfiiiA$G), max(dfiiiA$G), length.out=6),
                        B = seq(min(dfiiiA$B), max(dfiiiA$B), length.out=6),
                        Hue = seq(min(dfiiiA$Hue), max(dfiiiA$Hue), length.out=6),
                        Lightness = unique(dfiiiA$Lightness),
                        Saturation = unique(dfiiiA$Saturation),
                        KEEP.OUT.ATTRS = FALSE,
                        stringsAsFactors = FALSE) %>% 
  as.data.frame() %>% tibble::as_tibble()
```

The code chunk below create a function to generate posterior samples of $\beta$
```{r}
generate_glm_post_samples <- function(mvn_result, num_samples)
{
  # specify the number of unknown beta parameters
  length_beta <- length(mvn_result$mode)

  # generate the random samples
  beta_samples <-  MASS::mvrnorm(n = num_samples, mu = mvn_result$mode, Sigma = mvn_result$var_matrix)
  
  # change the data type and name
  beta_samples %>% 
    as.data.frame() %>% tibble::as_tibble() %>% 
    purrr::set_names(sprintf("beta_%02d", (1:length_beta) - 1))
}
```

The code chunk below creates matrices of linear predictor $\eta$ and average output $\mu$
```{r}
post_logistic_pred_samples <- function(Xnew, Bmat)
{
  # calculate the linear predictor at all prediction points and posterior samples
  eta_mat <- Xnew%*%t(Bmat)
  
  # calculate the event probability
  mu_mat <- boot::inv.logit(eta_mat)
  
  # book keeping
  list(eta_mat = eta_mat, mu_mat = mu_mat)
}
```

Below is a function to summarize the posterior predictions
```{r}
summarize_logistic_pred_from_laplace <- function(mvn_result, Xtest, num_samples)
{
  # generate posterior samples of the beta parameters
  betas <- generate_glm_post_samples(mvn_result, num_samples)

  # data type conversion
  betas <- as.matrix(betas)
  
  # make posterior predictions on the test set
  pred_test <- post_logistic_pred_samples(Xtest, betas)
  
  # calculate summary statistics on the posterior predicted probability
  # summarize over the posterior samples
  
  mu_avg <- rowMeans(pred_test$mu_mat)
  
  # posterior quantiles
  mu_q05 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.05)
  mu_q95 <- apply(pred_test$mu_mat, 1, stats::quantile, probs = 0.95)
  
  # book keeping
  tibble::tibble(
    mu_avg = mu_avg,
    mu_q05 = mu_q05,
    mu_q95 = mu_q95
  ) %>% 
    tibble::rowid_to_column("pred_id")
}
```

The code chunk below creates test design matrices with viz_grid
```{r}
Xviz_8 <- model.matrix(~splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2)+Saturation, data = viz_grid)

Xviz_10 <- model.matrix(~splines::ns(R, df=5)+splines::ns(G, df=5)+splines::ns(B, df=5)+splines::ns(Hue, df=5)+Lightness+Saturation, data = viz_grid)
```

The code chunk below make posterior predictions for 2500 samples and summarizes to give average trend and 90% confidence interval
```{r}
set.seed(8123) 

post_pred_summary_8 <- summarize_logistic_pred_from_laplace(laplace_glm_08, Xviz_8, 2500)

post_pred_summary_10 <- summarize_logistic_pred_from_laplace(laplace_glm_10, Xviz_10, 2500)
```

Below is a function to visualize the predictions
```{r}
viz_bayes_logpost_preds <- function(post_pred_summary, input_df)
{
  post_pred_summary %>% 
    left_join(input_df %>% tibble::rowid_to_column('pred_id'),
              by = 'pred_id') %>% 
    ggplot(mapping = aes(x = Hue)) +
    geom_ribbon(mapping = aes(ymin = mu_q05,
                              ymax = mu_q95),
                alpha = 0.25) +
    geom_line(mapping = aes(y = mu_avg),
              size = 1.15) +
    facet_wrap( ~ G, labeller = 'label_both') +
    labs(y = "event probability") +
    theme_bw()
}
```

The code chunk below visualizes the created summary for model 8
```{r}
viz_bayes_logpost_preds(post_pred_summary_8,viz_grid)
```

The code chunk below visualizes the created summary for model 10
```{r}
viz_bayes_logpost_preds(post_pred_summary_10,viz_grid)
```

Below is a function to visualize the predictions by grouping Saturation
```{r}
viz_bayes_logpost_preds_color <- function(post_pred_summary, input_df)
{
  post_pred_summary %>% 
    left_join(input_df %>% tibble::rowid_to_column('pred_id'),
              by = 'pred_id') %>% 
    ggplot(mapping = aes(x = Hue)) +
    geom_ribbon(mapping = aes(ymin = mu_q05,
                              ymax = mu_q95,
                              fill = Saturation),
                alpha = 0.25) +
    geom_line(mapping = aes(y = mu_avg,
                            color=Saturation),
              size = 1.15) +
    facet_wrap( ~ G, labeller = 'label_both') +
    labs(y = "event probability") +
    theme_bw()
}
```

The code chunk below visualizes the created summary for model 8
```{r}
viz_bayes_logpost_preds_color(post_pred_summary_8,viz_grid)
```

The code chunk below visualizes the created summary for model 10
```{r}
viz_bayes_logpost_preds_color(post_pred_summary_10,viz_grid)
```

Below is a function to visualize the predictions by adding interaction with Saturation
```{r}
viz_bayes_logpost_preds_interaction <- function(post_pred_summary, input_df)
{
  post_pred_summary %>% 
    left_join(input_df %>% tibble::rowid_to_column('pred_id'),
              by = 'pred_id') %>% 
    ggplot(mapping = aes(x = Hue)) +
    geom_ribbon(mapping = aes(ymin = mu_q05,
                              ymax = mu_q95,
                              group = interaction(Saturation, G),
                              fill = Saturation),
                alpha = 0.25) +
    geom_line(mapping = aes(y = mu_avg,
                            group = interaction(Saturation, G),
                            color = Saturation),
              size = 1.15) +
    facet_wrap( ~ G, labeller = 'label_both') +
    labs(y = "event probability") +
    theme_bw()
}
```

The code chunk below visualizes the created summary for model 8
```{r}
viz_bayes_logpost_preds_interaction(post_pred_summary_8,viz_grid)
```

The code chunk below visualizes the created summary for model 10
```{r}
viz_bayes_logpost_preds_interaction(post_pred_summary_10,viz_grid)
```

The trends are different for both the models

# iii D)

The dataset iiiA) has 0/1 encoding. However, `caret` prefer a different encoding.  Therefore, a different data set is made which is associated with iiiD) with changed data type of the `outcome` variable. The `ifelse()` function is used to convert `outcome` to a character data type. The value of `outcome = 1` is converted to the string `'event'` and the value of `outcome = 0` is converted to `'non_event'`. The `outcome` data type is then converted to a factor (R's categorical variable data type) with `'event'` forced as the first level.  

```{r, make_iiiD_data}
dfiiiD <- df %>% 
  dplyr::select(-response) %>% 
  dplyr::mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))

dfiiiD %>% glimpse()
```

By converting `outcome` to a factor, the unique values of the variables are "always known":  
```{r, show_outcome_levels}
dfiiiD %>% pull(outcome) %>% levels()
```

However, the value counts are the same as the original encoding.  
```{r, confirm_outcome_Counts}
dfiiiD %>% dplyr::count(outcome)
```

Load essential libraries
```{r}
library(caret)
library(yardstick)
```

## Model Training and Validation Using ROC

Initializing metrics required.
```{r}
glm_ctrl_roc <- trainControl(method = "repeatedcv", number = 5, repeats = 5, summaryFunction=twoClassSummary,  classProbs = TRUE, savePredictions = TRUE)
glm_metric_roc <- "ROC"
```

### Training Model-1

Model with linear additive features

```{r}
set.seed(1001)
fit_glm_1_roc <- train(outcome ~ .,
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_1_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_1_roc
```{r}
fit_glm_1_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_1_roc.
```{r}
coefplot::coefplot(fit_glm_1_roc)
```

The code chunk below visualizes the important features for the trained model, fit_glm_1_roc
```{r}
varImp(fit_glm_1_roc)
```

### Training Model-2

Model with categorical inputs added to all main effect and all pairwise interactions of continuous inputs

```{r}
set.seed(1001)
fit_glm_2_roc <- train(outcome ~ Lightness + Saturation + ((R + G + B + Hue)^2),
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_2_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_2_roc
```{r}
fit_glm_2_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_2_roc.
```{r}
coefplot::coefplot(fit_glm_2_roc)
```

The code chunk below displays the important features for the trained model, fit_glm_2_roc
```{r}
varImp(fit_glm_2_roc)
```

### Training Model-3

Model with natural splines of 2 degrees of freedom of  G, B and Hue interaction added to Saturation, the best model from part-A

```{r}
set.seed(1001)
fit_glm_3_roc <- train(outcome ~ splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2) + (Saturation),
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_3_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_3_roc
```{r}
fit_glm_3_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_3_roc.
```{r}
coefplot::coefplot(fit_glm_3_roc)
```

The code chunk below displays the important features for the trained model, fit_glm_3_roc
```{r}
varImp(fit_glm_3_roc)
```

### Training Model-4

Model with splines of degree 5 added to each other and to categorical variables, the 2nd model selected from part A

```{r}
set.seed(1001)
fit_glm_4_roc <- train(outcome ~ splines::ns(R, df=5)+splines::ns(G, df=5)+splines::ns(B, df=5)+splines::ns(Hue, df=5)+Lightness+Saturation,
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_4_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_4_roc
```{r}
fit_glm_4_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_4_roc.
```{r}
coefplot::coefplot(fit_glm_4_roc)
```

The code chunk below displays the important features for the trained model, fit_glm_4_roc
```{r}
varImp(fit_glm_4_roc)
```

### Training Model-5 (elastic net)

Regularised model with categorical inputs added to all main effect and all pairwise interactions of continuous inputs

```{r}
set.seed(1001)
fit_glm_5_roc <- train(outcome ~ Lightness + Saturation + ((R + G + B + Hue)^2),
                  data = dfiiiD ,
                  method = "glmnet",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_5_roc
```

```{r}
enet_grid_glm_1_roc <- expand.grid(alpha = seq(0.1, 1, by = 0.1),
                         lambda = exp(seq(log(min(fit_glm_5_roc$results$lambda)),log(max(fit_glm_5_roc$results$lambda)), length.out = 25)),
                         KEEP.OUT.ATTRS = FALSE,
                         stringsAsFactors = FALSE)
enet_grid_glm_1_roc%>%glimpse()
```
The code chunk below trains, assess, and tunes the elastic net model with the custom tuning grid
```{r}
set.seed(1001)
enet_glm_1_tune_roc <- train(outcome~Lightness + Saturation + ((R + G + B + Hue)^2), data = dfiiiD , method='glmnet', metric = glm_metric_roc, preProcess=c("center", "scale"), trControl = glm_ctrl_roc, family="binomial", tuneGrid=enet_grid_glm_1_roc, xTrans = "log" )
```

```{r}
enet_glm_1_tune_roc$bestTune
```

The code chunk below visualizes the resampling results of the tuned model
```{r}
plot(enet_glm_1_tune_roc, xTrans = log)
```

The code chunks below compares the performance of regularized and tuned model

```{r}
enet_glm_1_results_roc <- resamples(list(mod_1 = fit_glm_5_roc,
                             mod_2 = enet_glm_1_tune_roc
                             ))
```

```{r}
summary(enet_glm_1_results_roc)
```

```{r}
dotplot(enet_glm_1_results_roc)
```

The code chunk below creates the roc plot for the tuned model, enet_glm_1_tune_roc
```{r}
enet_glm_1_tune_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the tuned model, enet_glm_1_tune_roc
```{r}
coefplot::coefplot(enet_glm_1_tune_roc$finalModel, enet_glm_1_tune_roc$bestTune$lambda)
```

The code chunk below displays the important features for the trained model, enet_glm_1_tune_roc
```{r}
varImp(enet_glm_1_tune_roc)
```

### Training Model-6(elastic net)

Regularized Model with natural splines of 2 degrees of freedom of  G, B and Hue interaction added to Saturation

```{r}
set.seed(1001)
fit_glm_6_roc <- train(outcome ~ splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2)+(Saturation),
                  data = dfiiiD ,
                  method = "glmnet",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_6_roc
```

The below chunk creates a custom tuning grid to further tune the elastic net `lambda` and `alpha` tuning parameters
```{r}
enet_grid_glm_2_roc <- expand.grid(alpha = seq(0.1, 1, by = 0.1),
                         lambda = exp(seq(log(min(fit_glm_6_roc$results$lambda)),log(max(fit_glm_6_roc$results$lambda)), length.out = 25)),
                         KEEP.OUT.ATTRS = FALSE,
                         stringsAsFactors = FALSE)
enet_grid_glm_2_roc%>%glimpse()
```

The code chunk below trains, assess, and tunes the elastic net model with the custom tuning grid
```{r}
set.seed(1001)
enet_glm_2_tune_roc <- train(outcome~splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2)+(Saturation), data = dfiiiD , method='glmnet', metric = glm_metric_roc, preProcess=c("center", "scale"), trControl = glm_ctrl_roc, family="binomial", tuneGrid=enet_grid_glm_2_roc, xTrans = "log" )
```

```{r}
enet_glm_2_tune_roc$bestTune
```

The code chunk below visualizes the resampling results of the tuned model
```{r}
plot(enet_glm_2_tune_roc, xTrans = log)
```

The code chunks below compares the performance of regularized and tuned model

```{r}
enet_glm_2_results_roc <- resamples(list(mod_1 = fit_glm_6_roc,
                             mod_2 = enet_glm_2_tune_roc
                             ))
```

```{r}
summary(enet_glm_2_results_roc)
```

```{r}
dotplot(enet_glm_2_results_roc)
```

The code chunk below creates the roc plot for the tuned model, enet_glm_2_tune_roc
```{r}
enet_glm_2_tune_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below visualizes the coefficient plot for the tuned model, enet_glm_2_tune_roc
```{r}
coefplot::coefplot(enet_glm_2_tune_roc$finalModel)
```

The code chunk below displays the important features for the trained model, enet_glm_2_tune_roc
```{r}
varImp(enet_glm_2_tune_roc)
```

### Training Model-7 (Neural Network)

```{r}
set.seed(1001)
fit_glm_nnet_roc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "nnet",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc, 
                  family="binomial", 
                  trace = FALSE)
fit_glm_nnet_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_nnet_roc
```{r}
fit_glm_nnet_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below displays the important features for the trained model, fit_glm_nnet_roc
```{r}
varImp(fit_glm_nnet_roc)
```

### Training Model-8 (Random Forest)

```{r}
set.seed(1001)
fit_glm_rf_roc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "rf",
                  metric = glm_metric_roc,
                  trControl = glm_ctrl_roc, 
                  family="binomial", 
                  trace = FALSE, 
                  importance=TRUE)
fit_glm_rf_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_rf_roc
```{r}
fit_glm_rf_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below displays the important features for the trained model, fit_glm_rf_roc
```{r}
varImp(fit_glm_rf_roc)
```

### Training Model-9(Gradient Boosted Tree)

```{r}
set.seed(1001)
fit_glm_xgb_roc <- train(outcome ~ .,
                  data = dfiiiD ,
                  method = "xgbTree",
                  metric = glm_metric_roc,
                  trControl = glm_ctrl_roc,
                  family="binomial",
                  verbosity = 0)
fit_glm_xgb_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_xgb_roc
```{r}
fit_glm_xgb_roc$pred %>% 
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below displays the important features for the trained model, fit_glm_xgb_roc
```{r}
varImp(fit_glm_xgb_roc)
```

### Training Model-10(Partial Least Square)

```{r}
set.seed(1001)
fit_glm_pls_roc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "pls",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
fit_glm_pls_roc
```

The code chunk below creates the roc plot for the trained model, fit_glm_pls_roc
```{r}
fit_glm_pls_roc$pred%>%
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below displays the important features for the trained model, fit_glm_gam_roc
```{r}
varImp(fit_glm_pls_roc)
```

### Training Model-11 (Generalized Additive models (GAM))

```{r}
set.seed(1001)
fit_glm_gam_roc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "gam",
                  metric = glm_metric_roc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_roc,
                  family="binomial")
```

The code chunk below creates the roc plot for the trained model, fit_glm_gam_roc
```{r}
fit_glm_gam_roc$pred%>%
  roc_curve(obs, event)%>%
  autoplot()
```

The code chunk below displays the important features for the trained model, fit_glm_gam_roc
```{r}
varImp(fit_glm_gam_roc$finalModel)
```

## Performance Comparison with Resampling

```{r}
glm_results_roc <- resamples(list(GLM_1 = fit_glm_1_roc,
                             GLM_2 = fit_glm_2_roc,
                             GLM_3 = fit_glm_3_roc,
                             GLM_4 = fit_glm_4_roc,
                             enet_1 = enet_glm_1_tune_roc,
                             enet_2 = enet_glm_2_tune_roc,
                             NNET = fit_glm_nnet_roc,
                             PLS = fit_glm_pls_roc,
                             RF = fit_glm_rf_roc,
                             XGB = fit_glm_xgb_roc,
                             GAM = fit_glm_gam_roc))
```

```{r}
summary(glm_results_roc)
```

```{r}
dotplot(glm_results_roc)
```

The best performance is given by Gradient Boosted Tree.
The code chunk below visualizes the importance of variables for the model
```{r}
plot(varImp(fit_glm_xgb_roc))
```

## Model Training and Validation Using Accuracy

```{r}
glm_ctrl_acc <- trainControl(method = "repeatedcv", number = 5, repeats = 5, savePredictions = TRUE)
glm_metric_acc <- "Accuracy"
```

### Training Model-1

Model with linear additive features

```{r}
set.seed(1001)
fit_glm_1_acc <- train(outcome ~ .,
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_1_acc
```

The code chunk below visualizes the confusion matrix of the tuned model, fit_glm_1_acc
```{r}
confusionMatrix(fit_glm_1_acc)
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_1_acc
```{r}
coefplot::coefplot(fit_glm_1_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_1_acc
```{r}
varImp(fit_glm_1_acc)
```

### Training Model-2

Model with categorical inputs added to all main effect and all pairwise interactions of continuous inputs

```{r}
set.seed(1001)
fit_glm_2_acc <- train(outcome ~ Lightness + Saturation + ((R + G + B + Hue)^2),
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_2_acc
```

The code chunk below visualizes the confusion matrix of the tuned model, fit_glm_2_acc
```{r}
confusionMatrix(fit_glm_2_acc)
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_2_acc
```{r}
coefplot::coefplot(fit_glm_2_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_2_acc
```{r}
varImp(fit_glm_2_acc)
```

### Training Model-3

Model with natural splines of 2 degrees of freedom of  G, B and Hue interaction added to Saturation, the best model from part A

```{r}
set.seed(1001)
fit_glm_3_acc <- train(outcome ~ splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2) + (Saturation),
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_3_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_3_acc
```{r}
confusionMatrix(fit_glm_3_acc)
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_3_acc
```{r}
coefplot::coefplot(fit_glm_3_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_3_acc
```{r}
varImp(fit_glm_3_acc)
```

### Training Model-4

Model with splines of degree 5 added to each other and to categorical variables, the 2nd model selected from part A

```{r}
set.seed(1001)
fit_glm_4_acc <- train(outcome ~ splines::ns(R, df=5)+splines::ns(G, df=5)+splines::ns(B, df=5)+splines::ns(Hue, df=5)+Lightness+Saturation,
                  data = dfiiiD ,
                  method = "glm",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_4_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_4_acc
```{r}
confusionMatrix(fit_glm_4_acc)
```

The code chunk below visualizes the coefficient plot for the trained model, fit_glm_4_acc
```{r}
coefplot::coefplot(fit_glm_4_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_4_acc
```{r}
varImp(fit_glm_4_acc)
```

### Training Model-5 (elastic net)

Regularised model with categorical inputs added to all main effect and all pairwise interactions of continuous inputs

```{r}
set.seed(1001)
fit_glm_5_acc <- train(outcome ~ Lightness + Saturation + ((R + G + B + Hue)^2),
                  data = dfiiiD ,
                  method = "glmnet",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_5_acc
```

The below chunk creates a custom tuning grid to further tune the elastic net `lambda` and `alpha` tuning parameters
```{r}
enet_grid_glm_1_acc <- expand.grid(alpha = seq(0.1, 1, by = 0.1),
                         lambda = exp(seq(log(min(fit_glm_5_acc$results$lambda)),log(max(fit_glm_5_acc$results$lambda)), length.out = 25)),
                         KEEP.OUT.ATTRS = FALSE,
                         stringsAsFactors = FALSE)
enet_grid_glm_1_acc%>%glimpse()
```

The code chunk below trains, assess, and tunes the elastic net model with the custom tuning grid
```{r}
set.seed(1001)
enet_glm_1_tune_acc <- train(outcome~Lightness + Saturation + ((R + G + B + Hue)^2), data = dfiiiD , method='glmnet', metric = glm_metric_acc, preProcess=c("center", "scale"), trControl = glm_ctrl_acc, family="binomial", tuneGrid=enet_grid_glm_1_acc, xTrans = "log" )
```

```{r}
enet_glm_1_tune_acc$bestTune
```

The code chunk below visualizes the resampling results of the tuned model
```{r}
plot(enet_glm_1_tune_acc, xTrans = log)
```

The code chunks below compares the performance of regularized and tuned model

```{r}
enet_glm_1_results_acc <- resamples(list(mod_1 = fit_glm_5_acc,
                             mod_2 = enet_glm_1_tune_acc
                             ))
```

```{r}
summary(enet_glm_1_results_acc)
```

```{r}
dotplot(enet_glm_1_results_acc)
```

The code chunk below visualizes the confusion matrix of the tuned model, enet_glm_1_tune_acc
```{r}
confusionMatrix(enet_glm_1_tune_acc)
```

The code chunk below visualizes the coefficient plot for the tuned model, enet_glm_1_tune_acc
```{r}
coefplot::coefplot(enet_glm_1_tune_acc$finalModel)
```

The code chunk below displays the important features for the tuned model, enet_glm_1_tune_acc
```{r}
varImp(enet_glm_1_tune_acc)
```

### Training Model-6(elastic net)

Regularized Model with natural splines of 2 degrees of freedom of  G, B and Hue interaction added to Saturation

```{r}
set.seed(1001)
fit_glm_6_acc <- train(outcome ~ splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2)+(Saturation),
                  data = dfiiiD ,
                  method = "glmnet",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc,
                  family="binomial")
fit_glm_6_acc
```

The below chunk creates a custom tuning grid to further tune the elastic net `lambda` and `alpha` tuning parameters
```{r}
enet_grid_glm_2_acc <- expand.grid(alpha = seq(0.1, 1, by = 0.1),
                         lambda = exp(seq(log(min(fit_glm_6_acc$results$lambda)),log(max(fit_glm_6_acc$results$lambda)), length.out = 25)),
                         KEEP.OUT.ATTRS = FALSE,
                         stringsAsFactors = FALSE)
enet_grid_glm_2_acc%>%glimpse()
```

The code chunk below trains, assess, and tunes the elastic net model with the custom tuning grid
```{r}
set.seed(1001)
enet_glm_2_tune_acc <- train(outcome~splines::ns(G, df=2)*splines::ns(B, df=2)*splines::ns(Hue, df=2)+(Saturation), data = dfiiiD , method='glmnet', metric = glm_metric_acc, preProcess=c("center", "scale"), trControl = glm_ctrl_acc, family="binomial", tuneGrid=enet_grid_glm_2_acc, xTrans = "log" )
```

```{r}
enet_glm_2_tune_acc$bestTune
```

The code chunk below visualizes the resampling results of the tuned model
```{r}
plot(enet_glm_2_tune_acc, xTrans = log)
```

The code chunks below compares the performance of regularized and tuned model
```{r}
enet_glm_2_results_acc <- resamples(list(mod_1 = fit_glm_6_acc,
                             mod_2 = enet_glm_2_tune_acc
                             ))
```

```{r}
summary(enet_glm_2_results_acc)
```

```{r}
dotplot(enet_glm_2_results_acc)
```

The code chunk below visualizes the confusion matrix of the tuned model, enet_glm_2_tune_acc
```{r}
confusionMatrix(enet_glm_2_tune_acc)
```

The code chunk below visualizes the coefficient plot for the tuned model, enet_glm_2_tune_roc
```{r}
coefplot::coefplot(enet_glm_2_tune_acc$finalModel)
```

The code chunk below displays the important features for the tuned model, enet_glm_2_tune_acc
```{r}
varImp(enet_glm_2_tune_acc)
```

### Training Model-7 (Neural Network)

```{r}
set.seed(1001)
fit_glm_nnet_acc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "nnet",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc, family="binomial", trace = FALSE)
fit_glm_nnet_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_nnet_acc
```{r}
confusionMatrix(fit_glm_nnet_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_nnet_acc
```{r}
varImp(fit_glm_nnet_acc)
```

### Training Model-8 (Random Forest)

```{r}
set.seed(1001)
fit_glm_rf_acc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "rf",
                  metric = glm_metric_acc,
                  trControl = glm_ctrl_acc, family="binomial", trace = FALSE, importance=TRUE)
fit_glm_rf_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_rf_acc
```{r}
confusionMatrix(fit_glm_rf_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_rf_acc
```{r}
varImp(fit_glm_rf_acc)
```

### Training Model-9(Gradient Boosted Tree)

```{r}
set.seed(1001)
fit_glm_xgb_acc <- train(outcome ~ .,
                  data = dfiiiD ,
                  method = "xgbTree",
                  metric = glm_metric_acc,
                  trControl = glm_ctrl_acc, 
                  family="binomial",
                  verbosity = 0)
fit_glm_xgb_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_xgb_acc
```{r}
confusionMatrix(fit_glm_xgb_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_xgb_acc
```{r}
varImp(fit_glm_xgb_acc)
```

### Training Model-10(Support Vector Machines)

```{r}
set.seed(1001)
fit_glm_pls_acc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "pls",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc, family="binomial")
fit_glm_pls_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_pls_acc
```{r}
confusionMatrix(fit_glm_pls_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_pls_acc
```{r}
varImp(fit_glm_pls_acc)
```

### Training Model-11 (Generalized Additive model)

```{r}
set.seed(1001)
fit_glm_gam_acc <- train(outcome   ~ .,
                  data = dfiiiD ,
                  method = "gam",
                  metric = glm_metric_acc,
                  preProcess = c("center", "scale"),
                  trControl = glm_ctrl_acc, family="binomial")
fit_glm_gam_acc
```

The code chunk below visualizes the confusion matrix of the trained model, fit_glm_gam_acc
```{r}
confusionMatrix(fit_glm_gam_acc)
```

The code chunk below displays the important features for the trained model, fit_glm_gam_acc
```{r}
varImp(fit_glm_gam_acc)
```

## Performance Comparison with Resampling

```{r}
glm_results_acc <- resamples(list(GLM_1 = fit_glm_1_acc,
                             GLM_2 = fit_glm_2_acc,
                             GLM_3 = fit_glm_3_acc,
                             GLM_4 = fit_glm_4_acc,
                             enet_1 = enet_glm_1_tune_acc,
                             enet_2 = enet_glm_2_tune_acc,
                             NNET = fit_glm_nnet_acc,
                             PLS = fit_glm_pls_acc,
                             RF = fit_glm_rf_acc,
                             XGB = fit_glm_xgb_acc,
                             GAM = fit_glm_gam_acc))
```

```{r}
summary(glm_results_acc)
```

```{r}
dotplot(glm_results_acc)
```

Yes, the results are different when measuring performance with accuracy but the top model is the same

The best performance is given by Gradient Boosted Tree.
The code chunk below visualizes the importance of variables for the model

```{r}
plot(varImp(fit_glm_xgb_acc))
```

# Save the model with best performance

Save the model by piping the `fit_glm_xgb_roc` object into `readr::write_rds()`.
```{r}
fit_glm_xgb_roc %>% readr::write_rds("glm_model.rds")
```

# Predictions

```{r}
fit_glm_xgb_roc$pred%>%glimpse()
```

The code chunk below binds the predictions of probabilities made by summarizing through folds, trees etc to the dataset
```{r}
df_pred_glm <- cbind(dfiiiD,fit_glm_xgb_roc$pred %>% 
  dplyr::group_by(rowIndex) %>% 
  dplyr::summarise(pred = mean(event))%>%
  dplyr::mutate(event_prob = pred)%>%
  dplyr::mutate(pred = ifelse(event_prob>0.5, "event", "non_event"),pred = factor(pred, levels = c('event', 'non_event'))))
```

```{r}
df_pred_glm%>%glimpse()
```

Check the confusion matrix of predictions with the model

```{r}
confusionMatrix(factor(df_pred_glm$pred),factor(df_pred_glm$outcome))
```

```{r}
library(pROC)
```

```{r}
roc(df_pred_glm$outcome ~ df_pred_glm$event_prob, plot = TRUE, print.auc = TRUE)
```

# Save the predictions of best model (XGB)

```{r}
write.csv(df_pred_glm,file='glm_predict.csv')
```


